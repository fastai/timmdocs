---

title: How to train your own models using timm? 


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/01b_training.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/01b_training.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this tutorial, we will be looking at the training script of <code>timm</code>. There are various features that <code>timm</code> has to offer and some of them have been listed below:</p>
<ol>
<li>Auto Augmentation <a href="https://arxiv.org/abs/1805.09501">paper</a></li>
<li>Augmix </li>
<li>Distributed Training on multiple GPUs</li>
<li>Mixed precision training </li>
<li>Auxiliary Batch Norm for AdvProp <a href="https://arxiv.org/abs/1911.09665">paper</a></li>
<li>Synchronized Batch Norm </li>
<li>Mixup and Cutmix with an ability to switch between the two &amp; also turn-off augmentation at a certain epoch </li>
</ol>
<p><code>timm</code> also supports multiple optimizers &amp; schedulers. In this tutorial we will be only be looking at the above 7 features and look at how you could utilize <code>timm</code> to use these features for your own experiments on a custom dataset.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As part of this tutorial, we will first start out with a general introduction to the training script and look at the various key steps that occur inside this script at a high-level. Then, we will look at some of the details of the above 7 features to get a further understanding of <code>train.py</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-args">Training args<a class="anchor-link" href="#Training-args"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The training script in <code>timm</code> can accept ~100 arguments. You can find more about these by running <code>python train.py --help</code>. These arguments are to define Dataset/Model parameters, Optimizer parameters, Learnining Rate scheduler parameters, Augmentation and regularization, Batch Norm parameters, Model exponential moving average parameters, and some miscellaneaous parameters such as <code>--seed</code>, <code>--tta</code> etc.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As part of this tutorial, we will be looking at how the training script makes use of these parameters from a high-level view. This could be beneficial for you to able to run your own experiments on ImageNet or any other custom dataset using <code>timm</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Required-args">Required args<a class="anchor-link" href="#Required-args"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The only argument required by <code>timm</code> training script is the path to the training data such as ImageNet which is structured in the following way:</p>

<pre><code>imagenette2-320
├── train
│   ├── n01440764
│   ├── n02102040
│   ├── n02979186
│   ├── n03000684
│   ├── n03028079
│   ├── n03394916
│   ├── n03417042
│   ├── n03425413
│   ├── n03445777
│   └── n03888257
└── val
    ├── n01440764
    ├── n02102040
    ├── n02979186
    ├── n03000684
    ├── n03028079
    ├── n03394916
    ├── n03417042
    ├── n03425413
    ├── n03445777
    └── n03888257</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So to start training on this <code>imagenette2-320</code> we could just do something like <code>python train.py &lt;path_to_imagenette2-320_dataset&gt;</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Default-args">Default args<a class="anchor-link" href="#Default-args"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The various default args, in the training script are setup for you and what get's passed to the training script looks something like this:</p>
<div class="highlight"><pre><span></span><span class="n">Namespace</span><span class="p">(</span><span class="n">aa</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">amp</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">apex_amp</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">aug_splits</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">bn_eps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bn_momentum</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bn_tf</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">channels_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">clip_grad</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">color_jitter</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">cooldown_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">crop_pct</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">cutmix</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">cutmix_minmax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="s1">&#39;../imagenette2-320&#39;</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">decay_epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">dist_bn</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">drop</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">drop_block</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">drop_connect</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">drop_path</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">eval_metric</span><span class="o">=</span><span class="s1">&#39;top1&#39;</span><span class="p">,</span> <span class="n">gp</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hflip</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">img_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">initial_checkpoint</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">jsd</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">local_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">log_interval</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">lr_cycle_limit</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">lr_cycle_mul</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">lr_noise</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lr_noise_pct</span><span class="o">=</span><span class="mf">0.67</span><span class="p">,</span> <span class="n">lr_noise_std</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">mixup</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mixup_mode</span><span class="o">=</span><span class="s1">&#39;batch&#39;</span><span class="p">,</span> <span class="n">mixup_off_epoch</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mixup_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mixup_switch_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;resnet101&#39;</span><span class="p">,</span> <span class="n">model_ema</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">model_ema_decay</span><span class="o">=</span><span class="mf">0.9998</span><span class="p">,</span> <span class="n">model_ema_force_cpu</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">native_amp</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">no_aug</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">no_prefetcher</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">no_resume_opt</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">opt</span><span class="o">=</span><span class="s1">&#39;sgd&#39;</span><span class="p">,</span> <span class="n">opt_betas</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">opt_eps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">patience_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">pin_mem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ratio</span><span class="o">=</span><span class="p">[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.3333333333333333</span><span class="p">],</span> <span class="n">recount</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">recovery_interval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">remode</span><span class="o">=</span><span class="s1">&#39;const&#39;</span><span class="p">,</span> <span class="n">reprob</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">resplit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">resume</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">save_images</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="p">[</span><span class="mf">0.08</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="n">sched</span><span class="o">=</span><span class="s1">&#39;step&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">smoothing</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">split_bn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">start_epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sync_bn</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">torchscript</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">train_interpolation</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="n">train_split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">tta</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">use_multi_epochs_loader</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">val_split</span><span class="o">=</span><span class="s1">&#39;validation&#39;</span><span class="p">,</span> <span class="n">validation_batch_size_multiplier</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vflip</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">warmup_epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">warmup_lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
<p>Notice, that <code>args</code> is a <code>Namespace</code> which means we can set more along the way if needed by doing something like <code>args.new_variable="some_value"</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To get a one-line introduction of these various arguments, we can just do something like <code>python train.py --help</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='We will be looking at what these parameters in greater detail as part of this tutorial. Do note that some random augmentations are set by default such as <code>color_jitter</code>, <code>hfliip</code> but there is a parameter <code>no-aug</code> in case you wanted to turn of all training data augmentations. Also, the default optimizer <code>opt</code> is &#8217;sgd&#8217; but it is possible to change that. <code>timm</code> offers a vast number of optimizers to train your models with. ' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-training-script-in-20-steps">The training script in 20 steps<a class="anchor-link" href="#The-training-script-in-20-steps"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this section we will look at the various steps from a high level perspective that occur inside the training script. These steps have been outlined below in the correct order:</p>
<ol>
<li>Setup up <u>distributed training parameters</u> if <code>args.distributed</code> is <code>True</code>. </li>
<li>Setup <u>manual seed</u> for reproducible results. </li>
<li><strong>Create Model</strong>: Create the model to train using <code>timm.create_model</code> function. </li>
<li>Setup <u>data config</u> based on model's default config. In general the default config of the model looks something like: <div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;url&#39;</span><span class="p">:</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;num_classes&#39;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span> <span class="s1">&#39;input_size&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="s1">&#39;pool_size&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="s1">&#39;crop_pct&#39;</span><span class="p">:</span> <span class="mf">0.875</span><span class="p">,</span> <span class="s1">&#39;interpolation&#39;</span><span class="p">:</span> <span class="s1">&#39;bicubic&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">),</span> <span class="s1">&#39;std&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">),</span> <span class="s1">&#39;first_conv&#39;</span><span class="p">:</span> <span class="s1">&#39;conv1&#39;</span><span class="p">,</span> <span class="s1">&#39;classifier&#39;</span><span class="p">:</span> <span class="s1">&#39;fc&#39;</span><span class="p">}</span>
</pre></div>
</li>
<li>Setup <u>augmentation batch splits</u> and if the number of augmentation batch splits is more than 1, and if so, convert all model BatchNormlayers to Split Batch Normalization layers.
{% include note.html content='I feel this needs a little more explaination. In general, when we train a model, we apply the data augmentation to the complete batch and then define batch norm statistics from this complete batch such as mean and variance. But as introduced in this <a href="https://arxiv.org/abs/1911.09665">paper</a>, sometimes it is beneficial to split the data into groups and use separate Batch Normalization layers for each to normalize the groups independently throughout the training process. This is referred to as auxiliary batch norm in the paper and is referred to <code>SplitBatchNorm2d</code> in <code>timm</code>. ' %}{% include note.html content='Let&#8217;s assume that number of augmentation batch splits is set to two. In that case, we split the data into two groups - one without any augmentations (referred to as clean) and one with augmentations. Then we use two separate batch normalization layers to normalize the two groups throughout the training process. ' %}</li>
<li><p>If we are using multiple GPUs for training, then setup either apex syncBN or PyTorch native <a href="https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html">SyncBatchNorm</a> to set up <u>Synchronized Batch Normalization</u>. This means that rather than normalizing the data on each individual GPU, we normalize the whole batch at one spread across multiple GPUs.</p>
</li>
<li><p>Make model exportable using <code>torch.jit</code> if requested.</p>
</li>
<li><u>Initialize optimizer</u> based on arguments passed to the training script. </li>
<li>Setup <u>mixed Precision</u> - either using <code>apex.amp</code> or using native torch amp - <code>torch.cuda.amp.autocast</code>. </li>
<li>Load model weights if resuming from a <u>model checkpoint</u>.</li>
<li>Setup <u>exponential moving average</u> of model weights. This is similar to <a href="https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/">Stochastic Weight Averaging</a>. </li>
<li>Setup <u>distributed training</u> based on parameters from step-1.</li>
<li>Setup <u>learning rate scheduler</u>.</li>
<li>Create <u>training and validation dataset</u>.</li>
<li>Setup <u>Mixup/Cutmix</u> data augmentation.</li>
<li>Convert training dataset to <u>`AugmixDataset`</u> if number of augmentation batch splits from step-5 is greater than 1. </li>
<li>Create <u>training data loader and Validation dataloader</u>.
{% include note.html content='Transforms/Augmentations also get created inside the training dataloader.' %}18. Setup <u>Loss</u> function. </li>
<li>Setup <u>model checkpointing and evaluation metrics</u>. </li>
<li><u>Train and Validate</u> the model and also store the eval metrics to an output file.</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Some-key-timm-features">Some key <code>timm</code> features<a class="anchor-link" href="#Some-key-timm-features"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Auto-Augment">Auto-Augment<a class="anchor-link" href="#Auto-Augment"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To enable auto augmentation during training -</p>
<div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">./</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">aa</span> <span class="s1">&#39;v0&#39;</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Augmix">Augmix<a class="anchor-link" href="#Augmix"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A brief introduction about augmix has been presented <a href="https://fastai.github.io/timmdocs/dataset.html#AugmixDataset">here</a>. To enable augmix during training, simply do:</p>
<div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">./</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">aug</span><span class="o">-</span><span class="n">splits</span> <span class="mi">3</span> <span class="o">--</span><span class="n">jsd</span>
</pre></div>
<p><code>timm</code> also supports augmix with <code>RandAugment</code> and <code>AutoAugment</code> like so:</p>
<div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">./</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">aug</span><span class="o">-</span><span class="n">splits</span> <span class="mi">3</span> <span class="o">--</span><span class="n">jsd</span> <span class="o">--</span><span class="n">aa</span> <span class="n">rand</span><span class="o">-</span><span class="n">m9</span><span class="o">-</span><span class="n">mstd0</span><span class="mf">.5</span><span class="o">-</span><span class="n">inc1</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Distributed-Training-on-multiple-GPUs">Distributed Training on multiple GPUs<a class="anchor-link" href="#Distributed-Training-on-multiple-GPUs"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To train models on multiple GPUs, simply replace <code>python train.py</code> with <code>./distributed_train.sh &lt;num-gpus&gt;</code> like so:</p>
<div class="highlight"><pre><span></span><span class="o">./</span><span class="n">distributed_train</span><span class="o">.</span><span class="n">sh</span> <span class="mi">4</span> <span class="o">./</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">aug</span><span class="o">-</span><span class="n">splits</span> <span class="mi">3</span> <span class="o">--</span><span class="n">jsd</span>
</pre></div>
<p>This trains the model using <code>AugMix</code> data augmentation on 4 GPUs.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Mixed-precision-training">Mixed precision training<a class="anchor-link" href="#Mixed-precision-training"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To enable mixed precision training, simply add the <code>--amp</code> flag. <code>timm</code> will automatically implement mixed precision training either using <code>apex</code> or PyTorch Native mixed precision training.</p>
<div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">../</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">aug</span><span class="o">-</span><span class="n">splits</span> <span class="mi">3</span> <span class="o">--</span><span class="n">jsd</span> <span class="o">--</span><span class="n">amp</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Auxiliary-Batch-Norm/-SplitBatchNorm">Auxiliary Batch Norm/ <code>SplitBatchNorm</code><a class="anchor-link" href="#Auxiliary-Batch-Norm/-SplitBatchNorm"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>From the paper,</p>
<div class="highlight"><pre><span></span>Batch normalization serves as an essential component for many state-of-the-art computer vision models. Specifically, BN normalizes input features by the mean and variance computed within each mini-batch. <span class="gs">**One intrinsic assumption of utilizing BN is that the input features should come from a single or similar distributions.**</span> This normalization behavior could be problematic if the mini-batch contains data from different distributions, there- fore resulting in inaccurate statistics estimation.

To disentangle this mixture distribution into two simpler ones respectively for the clean and adversarial images, we hereby propose an auxiliary BN to guarantee its normalization statistics are exclusively preformed on the adversarial examples.
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To enable split batch norm,</p>
<div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">./</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">aug</span><span class="o">-</span><span class="n">splits</span> <span class="mi">3</span> <span class="o">--</span><span class="n">aa</span> <span class="n">rand</span><span class="o">-</span><span class="n">m9</span><span class="o">-</span><span class="n">mstd0</span><span class="mf">.5</span><span class="o">-</span><span class="n">inc1</span> <span class="o">--</span><span class="n">split</span><span class="o">-</span><span class="n">bn</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using the above command, <code>timm</code> now has separate batch normalization layer for each augmentation split.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Synchronized-Batch-Norm">Synchronized Batch Norm<a class="anchor-link" href="#Synchronized-Batch-Norm"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Synchronized batch norm is only used when training on multiple GPUs. From <a href="https://paperswithcode.com/method/syncbn">papers with code</a>:</p>
<div class="highlight"><pre><span></span>Synchronized Batch Normalization (SyncBN) is a type of batch normalization used for multi-GPU training. Standard batch normalization only normalizes the data within each device (GPU). SyncBN normalizes the input within the whole mini-batch.
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To enable, simply add <code>--sync-bn</code> flag like so:</p>
<div class="highlight"><pre><span></span><span class="o">./</span><span class="n">distributed_train</span><span class="o">.</span><span class="n">sh</span> <span class="mi">4</span> <span class="o">../</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">aug</span><span class="o">-</span><span class="n">splits</span> <span class="mi">3</span> <span class="o">--</span><span class="n">jsd</span> <span class="o">--</span><span class="n">sync</span><span class="o">-</span><span class="n">bn</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Mixup-and-Cutmix">Mixup and Cutmix<a class="anchor-link" href="#Mixup-and-Cutmix"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To enable either mixup or cutmix, simply add the <code>--mixup</code> or <code>--cutmix</code> flag with alpha value.<br>
Default probability of applying the augmentation is 1.0. If you need to change it, use <code>--mixup-prob</code> argument with new value.</p>
<p>For example, to enable mixup,</p>
<div class="highlight"><pre><span></span><span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">../</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">mixup</span> <span class="mf">0.5</span>
<span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">../</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">mixup</span> <span class="mf">0.5</span> <span class="o">--</span><span class="n">mixup</span><span class="o">-</span><span class="n">prob</span> <span class="mf">0.7</span>
</pre></div>
<p>Or for Cutmix,</p>
<div class="highlight"><pre><span></span><span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">../</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">cutmix</span> <span class="mf">0.5</span>
<span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">../</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">cutmix</span> <span class="mf">0.5</span> <span class="o">--</span><span class="n">mixup</span><span class="o">-</span><span class="n">prob</span> <span class="mf">0.7</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is also possible to enable both,</p>
<div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">../</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">mixup</span> <span class="mf">0.5</span> <span class="o">--</span><span class="n">cutmix</span> <span class="mf">0.5</span> <span class="o">--</span><span class="n">mixup</span><span class="o">-</span><span class="n">switch</span><span class="o">-</span><span class="n">prob</span> <span class="mf">0.3</span>
</pre></div>
<p>The above command will use either Mixup or Cutmix as data augmentation techniques and apply it to the batch with 50% probability. It will also switch between the two with 30% probability (Mixup - 70%, 30% switch to Cutmix).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There is also a parameter to turn off Mixup/Cutmix augmentation at a certail epoch:</p>
<div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">../</span><span class="n">imagenette2</span><span class="o">-</span><span class="mi">320</span> <span class="o">--</span><span class="n">mixup</span> <span class="mf">0.5</span> <span class="o">--</span><span class="n">cutmix</span> <span class="mf">0.5</span> <span class="o">--</span><span class="n">mixup</span><span class="o">-</span><span class="n">switch</span><span class="o">-</span><span class="n">prob</span> <span class="mf">0.3</span> <span class="o">--</span><span class="n">mixup</span><span class="o">-</span><span class="n">off</span><span class="o">-</span><span class="n">epoch</span> <span class="mi">10</span>
</pre></div>
<p>The above command only applies the Mixup/Cutmix data augmentation for the first 10 epochs.</p>

</div>
</div>
</div>
</div>
 

