---

title: Schedulers


keywords: fastai
sidebar: home_sidebar



nb_path: "nbs/07_schedulers.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/07_schedulers.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In <code>timm</code>, essentially we have a total of six different schedulers:</p>
<ol>
<li><a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/scheduler/cosine_lr.py#L18">CosineLRScheduler</a> - <a href="https://arxiv.org/abs/1608.03983">SGDR: Stochastic Gradient Descent with Warm Restarts</a></li>
<li><a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/scheduler/tanh_lr.py#L18">TanhLRScheduler</a> -  <a href="https://arxiv.org/abs/1806.01593">Stochastic Gradient Descent with Hyperbolic-Tangent Decay on Classification</a></li>
<li><a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/scheduler/step_lr.py#L13">StepLRScheduler</a></li>
<li><a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/scheduler/plateau_lr.py#L12">PlateauLRScheduler</a></li>
<li><a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/scheduler/multistep_lr.py#L10">MultiStepLRScheduler</a></li>
<li><a href="https://github.com/rwightman/pytorch-image-models/blob/master/timm/scheduler/poly_lr.py#L18">PolyLRScheduler</a> - SGDR scheduler with Polynomial anneling function.</li>
</ol>
<p>In this tutorial we are going to look at each one of them in detail and also look at how we can train our models using these schedulers using the <code>timm</code> training script or use them as standalone schedulers for custom PyTorch training scripts.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Available-Schedulers">Available Schedulers<a class="anchor-link" href="#Available-Schedulers"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this section we will look at the various available schedulers in <code>timm</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="CosineLRScheduler---SGDR">CosineLRScheduler - SGDR<a class="anchor-link" href="#CosineLRScheduler---SGDR"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First, let's look at the <code>CosineLRScheduler</code> - <code>SGDR</code> scheduler also referred to as the <code>cosine</code> scheduler in <code>timm</code>.</p>
<p>The <code>SGDR</code> scheduler, or the <code>Stochastic Gradient Descent with Warm Restarts</code> scheduler schedules the learning rate using a cosine schedule but with a tweak. It resets the learning rate to the initial value after some number of epochs.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html alt="SGDR" width="500" max-width="500" file="/timmdocs/images/SGDR.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='Unlike the builtin PyTorch schedulers, this is intended to be consistently called at the END of each epoch, before incrementing the epoch count, to calculate next epoch&#8217;s value &amp; at the END of each optimizer update, after incrementing the update count, to calculate next update&#8217;s value.' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="StepLRScheduler">StepLRScheduler<a class="anchor-link" href="#StepLRScheduler"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>StepLRScheduler</code> is a basic step LR schedule with warmup, noise. 
{% include note.html content='PyTorch&#8217;s implementation does not support warmup or noise. ' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The schedule for <code>StepLR</code> annealing looks something like:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html alt="StepLR" width="500" max-width="500" file="/timmdocs/images/StepLR.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>After a certain number <code>decay_epochs</code>, the learning rate is updated to be <code>lr * decay_rate</code>. In the above <code>StepLR</code> schedule, <code>decay_epochs</code> is set to 30 and <code>decay_rate</code> is set to 0.5 with an initial <code>lr</code> of 1e-4.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="TanhLRScheduler---SGDR-with-Hyperbolic-Tangent-Decay.">TanhLRScheduler - SGDR with Hyperbolic-Tangent Decay.<a class="anchor-link" href="#TanhLRScheduler---SGDR-with-Hyperbolic-Tangent-Decay."> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>TanhLRScheduler</code> is also referred to as the <code>tanh</code> annealing. <code>tanh</code> stands for hyperbolic tangent decay. The annealing using this scheduler looks something like:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html alt="Tanh" width="500" max-width="500" file="/timmdocs/images/Tanh.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is similar to the <code>SGDR</code> in the sense that the learning rate is set to the initial <code>lr</code> after a certain number of epochs but the annealing is done using the <code>tanh</code> function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="PlateauLRScheduler">PlateauLRScheduler<a class="anchor-link" href="#PlateauLRScheduler"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This scheduler is based on PyTorch's <a href="https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#ReduceLROnPlateau">ReduceLROnPlateau</a> scheduler with possible warmup and noise. The basic idea is to track an eval metric and based on the evaluation metric's value, the <code>lr</code> is reduced using <code>StepLR</code> if the eval metric is stagnant for a certain number of epochs.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="MultiStepLRScheduler">MultiStepLRScheduler<a class="anchor-link" href="#MultiStepLRScheduler"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>MultiStepLRScheduler</code> is LR schedule with decay learning rate at given epochs, with warmup, noise.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The schedule for <code>MultiStepLRScheduler</code> annealing (with 'step' at 10, 30) looks something like:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html alt="StepLR" width="500" max-width="500" file="/timmdocs/images/MultistepLr.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the above <code>MultiStepLRScheduler</code> schedule, <code>decay_t=[10, 30]</code> and <code>decay_rate=0.5</code> with an initial <code>lr</code> of 0.1.\
At epoch number 10 and 30, the learning rate is updated to be <code>lr * decay_rate</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="PolyLRScheduler---SGDR-with-Polynomial-Decay.">PolyLRScheduler - SGDR with Polynomial Decay.<a class="anchor-link" href="#PolyLRScheduler---SGDR-with-Polynomial-Decay."> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>PolyLRScheduler</code> is also referred to as the <code>poly</code> annealing.\
<code>poly</code> stands for polynomial decay.\
The annealing using this scheduler looks something like:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include image.html alt="Tanh" width="500" max-width="500" file="/timmdocs/images/PolyLR.png" %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is similar to the <code>cosine</code> and <code>tanh</code>in the sense that the learning rate is set to the initial <code>lr</code> after a certain number of epochs but the annealing is done using the <code>poly</code> function.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Using-the-various-schedulers-in-the-timm-training-script">Using the various schedulers in the <code>timm</code> training script<a class="anchor-link" href="#Using-the-various-schedulers-in-the-timm-training-script"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is very easy to train our models using the <code>timm</code>'s training script. Essentially, we simply pass in a parameter using the <code>--sched</code> flag to specify which scheduler to use and the various hyperparameters alongside.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>For <code>SGDR</code>, we pass in <code>--sched cosine</code>. </li>
<li>For <code>PlatueLRScheduler</code> we pass in <code>--sched plateau</code>. </li>
<li>For <code>TanhLRScheduler</code>, we pass in <code>--sched tanh</code>.</li>
<li>For <code>StepLR</code>, we pass in <code>--sched step</code>.</li>
<li>For <code>MultiStepLRScheduler</code>, we pass in <code>--sched multistep</code>.</li>
<li>For <code>PolyLRScheduler</code>, we pass in <code>--sched poly</code>.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Thus the call to the training script looks something like:</p>
<div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">sched</span> <span class="n">cosine</span> <span class="o">--</span><span class="n">epochs</span> <span class="mi">200</span> <span class="o">--</span><span class="nb">min</span><span class="o">-</span><span class="n">lr</span> <span class="mf">1e-5</span> <span class="o">--</span><span class="n">lr</span><span class="o">-</span><span class="n">cycle</span><span class="o">-</span><span class="n">mul</span> <span class="mi">2</span> <span class="o">--</span><span class="n">lr</span><span class="o">-</span><span class="n">cycle</span><span class="o">-</span><span class="n">limit</span> <span class="mi">2</span>
</pre></div>

</div>
</div>
</div>
</div>
 

