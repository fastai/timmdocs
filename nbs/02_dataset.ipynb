{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main Dataset classes in the `timm` library: \n",
    "1. `ImageDataset`\n",
    "2. `IterableImageDataset`\n",
    "3. `AugMixDataset`\n",
    "\n",
    "In this piece of documentation, we will be looking at each one of them individually and also looking at various use cases for these Dataset classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class ImageDataset(root: str, parser: Union[ParserImageInTar, ParserImageFolder, str] = None, class_map: Dict[str, str] = '', load_bytes: bool = False, transform: List = None) -> Tuple[Any, Any]:\n",
    "```\n",
    "\n",
    "\n",
    "The `ImageDataset` can be used to create both training and validation datasets is very similar to [torchvision.datasets.ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) in it's functionality with some nice addons. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `parser` is set automatically using a `create_parser` factory method. The `parser` finds all images and targets in `root` where the `root` folder is structured like so: \n",
    "\n",
    "```markdown\n",
    "root/dog/xxx.png\n",
    "root/dog/xxy.png\n",
    "root/dog/xxz.png\n",
    "\n",
    "root/cat/123.png\n",
    "root/cat/nsdf3.png\n",
    "root/cat/asd932_.png\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `parser` sets a `class_to_idx` dictionary mapping from the classes to integers which looks something like: \n",
    "\n",
    "```python\n",
    "{'dog': 0, 'cat': 1, ..}\n",
    "```\n",
    "\n",
    "And also has an attribute called `samples` which is a List of Tuples that looks something like: \n",
    "\n",
    "```python \n",
    "[('root/dog/xxx.png', 0), ('root/dog/xxy.png', 0), ..., ('root/cat/123.png', 1), ('root/cat/nsdf3.png', 1), ...]\n",
    "```\n",
    "\n",
    "This `parser` object is subscriptable and on doing something like `parser[index]` it returns a sample at that particular index in `self.samples`. Therefore, doing something like `parser[0]` would return `('root/dog/xxx.png', 0)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__getitem__(index: int) → Tuple[Any, Any]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `parser` is set, then the `ImageDataset` get's an image, target from this `parser` based on the `index`. \n",
    "\n",
    "```python \n",
    "img, target = self.parser[index]\n",
    "```\n",
    "\n",
    "It then reads the image either as a `PIL.Image` and converts to `RGB` or reads the image as bytes depending on the `load_bytes` argument. \n",
    "\n",
    "Finally, it transforms the image and returns the target. A dummy target `torch.tensor(-1)` is returned in case target is None. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `ImageDataset` can also be used as a replacement for `torchvision.datasets.ImageFolder`. Considering we have the `imagenette2-320` dataset present whose structure looks like: \n",
    "\n",
    "```\n",
    "imagenette2-320\n",
    "├── train\n",
    "│   ├── n01440764\n",
    "│   ├── n02102040\n",
    "│   ├── n02979186\n",
    "│   ├── n03000684\n",
    "│   ├── n03028079\n",
    "│   ├── n03394916\n",
    "│   ├── n03417042\n",
    "│   ├── n03425413\n",
    "│   ├── n03445777\n",
    "│   └── n03888257\n",
    "└── val\n",
    "    ├── n01440764\n",
    "    ├── n02102040\n",
    "    ├── n02979186\n",
    "    ├── n03000684\n",
    "    ├── n03028079\n",
    "    ├── n03394916\n",
    "    ├── n03417042\n",
    "    ├── n03425413\n",
    "    ├── n03445777\n",
    "    └── n03888257\n",
    "```\n",
    "\n",
    "And each subfolder contains a set of `.JPEG` files belonging to that class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# run only once\n",
    "wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\n",
    "gunzip imagenette2-320.tgz\n",
    "tar -xvf imagenette2-320.tar\n",
    "```\n",
    "\n",
    "Then, it is possible to create an `ImageDataset` like so: \n",
    "\n",
    "```python\n",
    "from timm.data.dataset import ImageDataset\n",
    "\n",
    "dataset = ImageDataset('./imagenette2-320')\n",
    "dataset[0]\n",
    "\n",
    "(<PIL.Image.Image image mode=RGB size=426x320 at 0x7FF7F4880460>, 0)\n",
    "```\n",
    "\n",
    "We can also see the `dataset.parser` is an instance of `ParserImageFolder`:\n",
    "\n",
    "```python\n",
    "dataset.parser\n",
    "\n",
    "<timm.data.parsers.parser_image_folder.ParserImageFolder at 0x7ff7f4880d90>\n",
    "```\n",
    "\n",
    "Finally, let's have a look at the `class_to_idx` Dictionary mapping in parser: \n",
    "\n",
    "```python\n",
    "dataset.parser.class_to_idx\n",
    "\n",
    "{'n01440764': 0,\n",
    " 'n02102040': 1,\n",
    " 'n02979186': 2,\n",
    " 'n03000684': 3,\n",
    " 'n03028079': 4,\n",
    " 'n03394916': 5,\n",
    " 'n03417042': 6,\n",
    " 'n03425413': 7,\n",
    " 'n03445777': 8,\n",
    " 'n03888257': 9}\n",
    "```\n",
    "\n",
    "And, also, the first five samples like so: \n",
    "\n",
    "```python\n",
    "dataset.parser.samples[:5]\n",
    "\n",
    "[('./imagenette2-320/train/n01440764/ILSVRC2012_val_00000293.JPEG', 0),\n",
    " ('./imagenette2-320/train/n01440764/ILSVRC2012_val_00002138.JPEG', 0),\n",
    " ('./imagenette2-320/train/n01440764/ILSVRC2012_val_00003014.JPEG', 0),\n",
    " ('./imagenette2-320/train/n01440764/ILSVRC2012_val_00006697.JPEG', 0),\n",
    " ('./imagenette2-320/train/n01440764/ILSVRC2012_val_00007197.JPEG', 0)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IterableImageDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`timm` also provides an `IterableImageDataset` similar to PyTorch's [IterableDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset) but, with a key difference - the `IterableImageDataset` applies the transforms to `image` before it yields an image and a target. \n",
    "\n",
    ">  Such form of datasets are particularly useful when data come from a stream or when the length of the data is unknown. \n",
    "\n",
    "`timm` applies the transforms lazily to the `image` and also sets the target to a dummy target `torch.tensor(-1, dtype=torch.long)` in case the target is `None`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the `ImageDataset` above, the `IterableImageDataset` first creates a parser which gets a tuple of samples based on the `root` directory. \n",
    "\n",
    "As explained before, the parser returns an image and the target is the corresponding folder in which the image exists. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">NOTE: The `IterableImageDataset` does not have a `__getitem__` method defined therefore it is not subscriptable. Doing something like `dataset[0]` where the `dataset` is an instance of `IterableImageDataset` would return an error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__iter__`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__iter__` method inside `IterableImageDataset` first gets an image and a target from `self.parser` and then lazily applies the transforms to the image. Also, sets the target as a dummy value before both are returned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<_io.BufferedReader name='../../imagenette2-320/train/n01440764/ILSVRC2012_val_00000293.JPEG'>,\n",
       "  0),\n",
       " (<_io.BufferedReader name='../../imagenette2-320/train/n01440764/ILSVRC2012_val_00000293.JPEG'>,\n",
       "  0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timm.data import IterableImageDataset\n",
    "from timm.data.parsers.parser_image_folder import ParserImageFolder\n",
    "from timm.data.transforms_factory import create_transform \n",
    "\n",
    "root = '../../imagenette2-320/'\n",
    "parser = ParserImageFolder(root)\n",
    "iterable_dataset = IterableImageDataset(root=root, parser=parser)\n",
    "parser[0], next(iter(iterable_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `iterable_dataset` is not Subscriptable.\n",
    "\n",
    "```python\n",
    "iterable_dataset[0]\n",
    "\n",
    ">> \n",
    "---------------------------------------------------------------------------\n",
    "NotImplementedError                       Traceback (most recent call last)\n",
    "<ipython-input-14-9085b17eda0c> in <module>\n",
    "----> 1 iterable_dataset[0]\n",
    "\n",
    "~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataset.py in __getitem__(self, index)\n",
    "     30 \n",
    "     31     def __getitem__(self, index) -> T_co:\n",
    "---> 32         raise NotImplementedError\n",
    "     33 \n",
    "     34     def __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]':\n",
    "\n",
    "NotImplementedError: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AugmixDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class AugmixDataset(dataset: ImageDataset, num_splits: int = 2):\n",
    "```\n",
    "\n",
    "The `AugmixDataset` accepts an `ImageDataset` and converts it to an Augmix Dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's an Augmix Dataset and when would we need to do this?**\n",
    "\n",
    "Let's answer that with the help of the Augmix paper.\n",
    "\n",
    "<img alt=\"Augmix\" src=\"images/augmix.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the image above, the final `Loss Output` is actually the sum of classificaiton loss and `λ` times Jensen-Shannon loss between labels and model predictions on X<sub>orig</sub>, X<sub>augmix1</sub> and X<sub>augmix2</sub>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for such a case, we would require three versions of the batch - original, augmix1 and augmix2. So how we do achieve this? Using `AugmixDataset` ofcourse! \n",
    "\n",
    "> NOTE: `augmix1` and `augmix2` are the augmented versions of the original batch where the augmentations are chosen randomly from a list of Operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__getitem__(index: int) -> Tuple[Any, Any]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get an `X` and corresponding label `y` from the `self.dataset` which is the dataset passed into the `AugmixDataset` constructor. Next, we normalize this image `X` and add it to a variable called `x_list`. \n",
    "\n",
    "Next, based on the `num_splits` argument which defaults to 0, we apply `augmentations` to `X`, normalize the augmented output and append it to `x_list`. \n",
    "\n",
    "> NOTE: If `num_splits=2`, then `x_list` has two items - `original + augmented`. If `num_splits=3`, then `x_list` has three items - `original + augmented1 + augmented2`. And so on.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data import ImageDataset, IterableImageDataset, AugMixDataset, create_loader\n",
    "\n",
    "dataset = ImageDataset('../../imagenette2-320/')\n",
    "dataset = AugMixDataset(dataset, num_splits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = create_loader(\n",
    "    dataset, \n",
    "    input_size=(3, 224, 224), \n",
    "    batch_size=8, \n",
    "    is_training=True, \n",
    "    scale=[0.08, 1.], \n",
    "    ratio=[0.75, 1.33], \n",
    "    num_aug_splits=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Requires GPU to work\n",
    "\n",
    "next(iter(loader_train))[0].shape\n",
    "\n",
    ">> torch.Size([16, 3, 224, 224])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: Now at this stage, you might ask - we passed in `batch_size=8`, but the batch size returned by `loader_train` is 16? Why would that be?\n",
    "\n",
    "Because we passed in `num_aug_splits=2`. In this case, the `loader_train` has the first 8 original images and next 8 images that represent `augmix1`. \n",
    "\n",
    "Had we passed in `num_aug_splits=3`, then the effective `batch_size` would have been 24, where the first 8 images would have been the original images, next 8 representing `augmix1` and the last 8 representing `augmix2`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
